{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_3_Monte_Carlo_Search_TicTacTo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJ32qdBKWgj8BOXzkiKFV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonjun/AlphaZero/blob/master/5_3_Monte_Carlo_Search_TicTacTo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjIoevLsMizq"
      },
      "source": [
        "# 원시 몬테카를로 탐색이란?\n",
        "\n",
        "알파베타법 같은 경우 탐색의 가지를 쳐내어 틱택토와 같은 국면이 적은 게임이라면 문제가 없으나, 장기나 체스와 같이 국면이 많은 게임에서는 막대한 시간이 소요되기 때문에 현실적이지 않다.\n",
        "\n",
        "여기에서 수를 전개하는 부분을 더 줄여서 상태 가치를 계산하는 방법을 생각할 수 있다.\n",
        "\n",
        "이를 실현하는 방법으로는 '수제작 평가 함수'와 원시 몬테카를로 탐색(Monte Carlo Search), 몬테카를로 트리 탐색(Monte Carlo Tree Search)을 꼽을 수 있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jw5m26BNChA"
      },
      "source": [
        "* 직접 구현한 평가 함수\n",
        "\n",
        "'직접 구현한 평가 함수'란 장기에서 걸음 수가 4 이상이면 1점, 장기말이 움직일 수 있는 위치가 많은 1점과 같이 프로그래머가 스스로 계산 방법을 생각해 평가 함수를 만드는 방법이다. 강력한 평가 함수를 만들기 위해서는 우선 평가 함수를 작성하는 사람이 게임을 잘 알고 있어야 하며, 또한 그 지식을 알고리즘으로 만들어야 하기 때문에 난이도가 매우 높다.\n",
        "\n",
        "* 원시 몬테카를로 탐색, 몬테카를로 트리 탐색\n",
        "\n",
        "'원시 몬테카를로 탐색', '몬테카를로 트리 탐색'은 랜덤 시뮬레이션을 활용해 상태 가치를 계산하는 방법이다. 몬테카를로라는 말이 어렵게 들리지만, 이것은 랜덤이라는 의미를 말한다.\n",
        "\n",
        "현재 국면에서 게임 종료 시까지 계속해서 랜덤 플레이를 수행한 뒤, 승률이 높은 수가 가치가 높은 것으로 판단한다. 몬테카를로 트리 탐색은 원시 몬테카를로 탐색을 개선한 알고리즘이다.\n",
        "\n",
        "이번에는 원시 몬테카를로 탐색을 사용해볼 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVTRcJdPMYn7"
      },
      "source": [
        "# 틱택토 구현\n",
        "import random\n",
        "\n",
        "# 게임 상태\n",
        "class State:\n",
        "    # 초기화\n",
        "    def __init__(self, pieces=None, enemy_pieces=None):\n",
        "        # 돌 배치\n",
        "        self.pieces = pieces if pieces != None else [0] * 9\n",
        "        self.enemy_pieces = enemy_pieces if enemy_pieces != None else [0] * 9\n",
        "\n",
        "    # 돌의 수 취득\n",
        "    def piece_count(self, pieces):\n",
        "        count = 0\n",
        "        for i in pieces:\n",
        "            if i == 1:\n",
        "                count +=  1\n",
        "        return count\n",
        "\n",
        "    # 패배 여부 확인\n",
        "    def is_lose(self):\n",
        "        # 돌 3개 연결 여부\n",
        "        def is_comp(x, y, dx, dy):\n",
        "            for k in range(3):\n",
        "                if y < 0 or 2 < y or x < 0 or 2 < x or \\\n",
        "                    self.enemy_pieces[x+y*3] == 0:\n",
        "                    return False\n",
        "                x, y = x+dx, y+dy\n",
        "            return True\n",
        "\n",
        "        # 배패 여부 확인\n",
        "        if is_comp(0, 0, 1, 1) or is_comp(0, 2, 1, -1):\n",
        "            return True\n",
        "        for i in range(3):\n",
        "            if is_comp(0, i, 1, 0) or is_comp(i, 0, 0, 1):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # 무승부 여부 확인\n",
        "    def is_draw(self):\n",
        "        return self.piece_count(self.pieces) + self.piece_count(self.enemy_pieces) == 9\n",
        "\n",
        "    # 게임 종료 여부 확인\n",
        "    def is_done(self):\n",
        "        return self.is_lose() or self.is_draw()\n",
        "\n",
        "    # 다음 상태 얻기\n",
        "    def next(self, action):\n",
        "        pieces = self.pieces.copy()\n",
        "        pieces[action] = 1\n",
        "        return State(self.enemy_pieces, pieces)\n",
        "\n",
        "    # 합법적인 수의 리스트 얻기\n",
        "    def legal_actions(self):\n",
        "        actions = []\n",
        "        for i in range(9):\n",
        "            if self.pieces[i] == 0 and self.enemy_pieces[i] == 0:\n",
        "                actions.append(i)\n",
        "        return actions\n",
        "\n",
        "    # 선 수 여부 확인\n",
        "    def is_first_player(self):\n",
        "        return self.piece_count(self.pieces) == self.piece_count(self.enemy_pieces)\n",
        "\n",
        "    # 문자열 표시\n",
        "    def __str__(self):\n",
        "        ox = ('o', 'x') if self.is_first_player() else ('x', 'o')\n",
        "        str = ''\n",
        "        for i in range(9):\n",
        "            if self.pieces[i] == 1:\n",
        "                str += ox[0]\n",
        "            elif self.enemy_pieces[i] == 1:\n",
        "                str += ox[1]\n",
        "            else:\n",
        "                str += '-'\n",
        "            if i % 3 == 2:\n",
        "                str += '\\n'\n",
        "        return str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbkSmGh0Ocjq"
      },
      "source": [
        "# 랜덤으로 행동 선택( Mini-Max-Method)\n",
        "def random_action(state):\n",
        "    legal_actions = state.legal_actions()\n",
        "    return legal_actions[random.randint(0, len(legal_actions)-1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXr31dglOksQ"
      },
      "source": [
        "# 알파베타법을 활용한 상태 가치 계산\n",
        "def alpha_beta(state, alpha, beta):\n",
        "    # 패배 시, 상태 가치 -1\n",
        "    if state.is_lose():\n",
        "        return -1\n",
        "    \n",
        "    # 무승부 시, 상태 가치 0\n",
        "    if state.is_draw():\n",
        "        return  0\n",
        "\n",
        "    # 합법적인 수의 상태 가치 계산\n",
        "    for action in state.legal_actions():\n",
        "        score = -alpha_beta(state.next(action), -beta, -alpha)\n",
        "        if score > alpha:\n",
        "            alpha = score\n",
        "\n",
        "        # 현재 노드의 베스트 스코어가 새로운 노드보다 크면 탐색 종료\n",
        "        if alpha >= beta:\n",
        "            return alpha\n",
        "\n",
        "    # 합법적인 수의 상태 가치의 최대값을 반환\n",
        "    return alpha\n",
        "\n",
        "# 알파베타법을 활용한 행동 선택\n",
        "def alpha_beta_action(state):\n",
        "    # 합법적인 수의 상태 가치 계산\n",
        "    best_action = 0\n",
        "    alpha = -float('inf')\n",
        "    for action in state.legal_actions():\n",
        "        score = -alpha_beta(state.next(action), -float('inf'), -alpha)\n",
        "        if score > alpha:\n",
        "            best_action = action\n",
        "            alpha = score\n",
        "            \n",
        "    # 합법적인 수의 상태 가치값 중 최대값을 선택하는 행동 반환\n",
        "    return best_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj76k7hwOpuw"
      },
      "source": [
        "# 플레이아웃\n",
        "\n",
        "현재 국면에서 게임 종료까지 플레이하는 것을 플레이아웃(playout)이라고 부른다. 게임 종료까지 둘 수 있는 수를 랜덤으로 선택해서 상태 가치 '1: 승리', '-1: 패배', '0: 무승부'을 반환하는 함수를 만든다.\n",
        "\n",
        "체스나 장기의 경우, 랜덤으로 수를 선택하게 되면 승부가 날 때까지 매우 많은 수가 필요할 것이라 생각되는데, 실제로는 그렇게 오래 걸리지 않는다고 한다.\n",
        "\n",
        "예를 들면, 체스의 경우 임의의 국면에서의 둘 수 있는 수는 평균 35로, 평균 80수 정보에서 승부가 난다. 그러므로 모든 노드를 탐색하려면 35^80, 즉 10^120수 정도의 계산을 해야한다. 하지만 플레이아웃이라면 1회당 80수 정도의 계산만으로 완료할 수 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J91JQ3djRh4C"
      },
      "source": [
        "# 플레이아웃\n",
        "def playout(state):\n",
        "  # 패배 시, 상태 가치는 -1\n",
        "  if state.is_lose():\n",
        "    return -1\n",
        "  \n",
        "  # 무승부 시, 상태 가치는 0\n",
        "  if state.is_draw():\n",
        "    return 0\n",
        "\n",
        "  # 다음 상태의 상태 평가\n",
        "  return -playout(state.next(random_action(state)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmGKsG_ARy8B"
      },
      "source": [
        "# 원시 몬테카를로 탐색을 활용한 행동 선택\n",
        "\n",
        "원시 몬테카를로 탐색에서의 상태 가치 계산을 수행한다. 여기서는 둘 수 있는 수별로 10회 플레이아웃을 했을 때 상태 가치의 합계를 계산한다. 그리고 합계가 가장 큰 행동을 선택. 플레이아웃 횟수가 많을수록 정밀도가 높아지지만, 그만큼 시간도 오래 걸린다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBN6vEAqRyYb"
      },
      "source": [
        "# 원시 몬테카를로 탐색을 활용한 행동 선택\n",
        "def mcs_action(state):\n",
        "  # 합법적인 수 별로 10회 플레이아웃 시행 후, 상태 가치의 합계 계산\n",
        "  legal_actions = state.legal_actions()\n",
        "  values = [0] * len(legal_actions)\n",
        "  for i, action in enumerate(legal_actions):\n",
        "    for _ in range(10):\n",
        "      values[i] += -playout(state.next(action))\n",
        "    \n",
        "  # 합법적인 수의 상태 가치 합계의 최대값을 가지는 행동 반환\n",
        "  return legal_actions[argmax(values)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z96bFx_-S1JR"
      },
      "source": [
        "argmax()는 집합 중 최댓값의 인덱스를 반환하는 함수. 예를 들면, 'argmax([2,5,3])'이라면 최댓값은 5이므로 그 값의 인덱스인 1을 반환한다. 이 값을 사용해 상태 가치의 합계가 가장 큰 행동을 선택한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZQ-hML7S0xR"
      },
      "source": [
        "# 최대값의 인덱스를 반환\n",
        "def argmax(collection, key=None):\n",
        "  return collection.index(max(collection))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAVH1jSdTFcK"
      },
      "source": [
        "# 원시 몬테카를로 탐색과 랜덤 및 알파베타법의 대전\n",
        "\n",
        "원시 몬테카를로 탐색과 랜덤 및 알파베타법을 대전시킨다. 게임을 100회 플레이하고 그 승률을 표시한다. 또한 틱택토에서는 선 수가 유리하므로 선/후 수를 교대한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IqEvMgOTXFu",
        "outputId": "2201937a-c1f0-4c4d-cba4-d8bb40911358"
      },
      "source": [
        "# 파라미터\n",
        "EP_GAME_COUNT = 100 # 평가 1회당 게임 수\n",
        "\n",
        "# 선 수 플레이어 포인트\n",
        "def first_player_point(ended_state):\n",
        "  # 1: 선 수 플레이어 승리, 0: 선 수 플레이어 패배, 0.5: 무승부\n",
        "  if ended_state.is_lose():\n",
        "    return 0 if ended_state.is_first_player() else 1\n",
        "  return 0.5\n",
        "\n",
        "# 게임 실행\n",
        "def play(next_actions):\n",
        "  # 상태 생성\n",
        "  state = State()\n",
        "\n",
        "  # 게임 종료 시까지 반복\n",
        "  while True:\n",
        "    # 게임 종료 시\n",
        "    if state.is_done():\n",
        "      break\n",
        "    \n",
        "    # 행동 얻기\n",
        "    next_action = next_actions[0] if state.is_first_player() else next_actions[1]\n",
        "    action = next_action(state)\n",
        "\n",
        "    # 다음 상태 얻기\n",
        "    state = state.next(action)\n",
        "\n",
        "  # 선 수 플레이어 포인트 반환\n",
        "  return first_player_point(state)\n",
        "\n",
        "# 임의의 알고리즘 평가\n",
        "def evaluate_algorithm_of(label, next_actions):\n",
        "  # 여러 차레 대전 반복\n",
        "  total_point = 0\n",
        "  for i in range(EP_GAME_COUNT):\n",
        "    # 1 게임 실행\n",
        "    if i % 2 == 0:\n",
        "      total_point += play(next_actions)\n",
        "    else:\n",
        "      total_point += 1-play(list(reversed(next_actions)))\n",
        "    \n",
        "    # 출력\n",
        "    print('\\rEvaluate {}/{}'.format(i+1, EP_GAME_COUNT), end='')\n",
        "  print('')\n",
        "\n",
        "  # 평균 포인트 계산\n",
        "  average_point = total_point / EP_GAME_COUNT\n",
        "  print(label.format(average_point))\n",
        "\n",
        "\n",
        "# 랜덤과의 대전\n",
        "next_actions = (mcs_action, random_action)\n",
        "evaluate_algorithm_of('VS_Random {:.3f}', next_actions)\n",
        "\n",
        "# 알파베타법과의 대전\n",
        "next_actions = (mcs_action, alpha_beta_action)\n",
        "evaluate_algorithm_of('VS_AlphaBeta {:.3f}', next_actions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate 100/100\n",
            "VS_Random 0.935\n",
            "Evaluate 100/100\n",
            "VS_AlphaBeta 0.290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We8XvOoV2Oy"
      },
      "source": [
        "위 결과를 보면 원시 몬테카를로 검색은 랜덤과의 대전에는 압승이지만, 알파베타법과의 대전에서는 패배하는 것을 볼 수 있다. 틱택토에서는 재귀적으로 게임 종료 시까지 조사하는 알파베타법이 가장 강하기 때문에 다른 알고리즘으로 이기기엔 상당히 어렵다."
      ]
    }
  ]
}